#!/usr/bin/env python3
"""
æ³°æ‹‰ç‘äºšåœ°å›¾æŸ¥çœ‹å™¨ settings.js æ±‰åŒ–è„šæœ¬ (æœ€ç»ˆç‰ˆ v2)
æ”¯æŒå­—æ®µ: Name, Anchor, Variety åŠå…¶ä»–æ–‡æœ¬å­—æ®µ
"""

from __future__ import annotations

import re
import json
import time
import os
import sys
import hashlib
import shutil
import traceback
from datetime import datetime
from pathlib import Path

try:
    from openai import OpenAI
except ImportError:
    print("âœ— è¯·å…ˆå®‰è£… openai: pip install openai")
    sys.exit(1)

# ======================== é…ç½®åŒº ========================
API_KEY = os.environ.get("OPENAI_API_KEY", "")
BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.siliconflow.cn/v1")
MODEL = os.environ.get("MODEL", "deepseek-ai/DeepSeek-V3.2")

INPUT_FILE = "settings.js"
OUTPUT_FILE = "settings_cn.js"
CACHE_FILE = "translation_cache.json"
LOG_FILE = "translation_log.txt"
REVIEW_FILE = "translation_review.json"
BACKUP_DIR = "backups"

BATCH_SIZE = 50
MAX_RETRIES = 5
RETRY_BASE_DELAY = 3
REQUEST_INTERVAL = 1.5
MAX_TOKENS = 4096

TRANSLATION_MODE = "replace"

DRY_RUN = False
SKIP_CATEGORIES = []

# ---- éœ€è¦ç¿»è¯‘çš„å­—æ®µåŠå…¶åˆ†ç±»æ ‡ç­¾ ----
# æ ¼å¼: (JSå­—æ®µå, ç¼“å­˜åˆ†ç±»å‰ç¼€, ç¿»è¯‘æç¤ºè¯æè¿°)
TRANSLATABLE_FIELDS = [
    ("Name",    "",          "åç§°"),
    ("Anchor",  "Anchor",   "é”šç‚¹æ–¹ä½"),
    ("Variety", "Variety",  "å˜ä½“/æ ·å¼æè¿°"),
]

# ---- å›ºå®šè¯æ±‡è¡¨ï¼ˆä¸è°ƒAPIï¼Œç›´æ¥æ›¿æ¢ï¼‰----
# é€‚ç”¨äºå€¼åŸŸå°ä¸”ç¡®å®šçš„å­—æ®µï¼Œå¦‚ Anchor
STATIC_DICT = {
    # Anchor æ–¹ä½
    "Bottom":       "åº•éƒ¨",
    "Top":          "é¡¶éƒ¨",
    "Left":         "å·¦ä¾§",
    "Right":        "å³ä¾§",
    "Center":       "ä¸­é—´",
    "Wall":         "å¢™å£",
    "None":         "æ— ",
    "Ground":       "åœ°é¢",
    "Ceiling":      "å¤©èŠ±æ¿",
    "SolidSide":    "å®å¿ƒä¾§é¢",
    "AlternateTop": "å¤‡é€‰é¡¶éƒ¨",
}
# ========================================================


def get_message_content(message) -> str:
    if message.content:
        return message.content
    if hasattr(message, 'reasoning_content') and message.reasoning_content:
        return message.reasoning_content
    if hasattr(message, 'text') and message.text:
        return message.text
    return ""


class TranslationLogger:
    def __init__(self, log_file: str):
        self.log_file = log_file
        self.start_time = datetime.now()
        self.api_calls = 0
        self.api_failures = 0
        self.tokens_used = 0
        self._write(f"\n{'=' * 60}")
        self._write(f"ç¿»è¯‘ä¼šè¯å¼€å§‹: {self.start_time.isoformat()}")
        self._write(f"æ¨¡å‹: {MODEL}")
        self._write(f"{'=' * 60}")

    def _write(self, msg: str):
        timestamp = datetime.now().strftime("%H:%M:%S")
        line = f"[{timestamp}] {msg}"
        print(line)
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(line + "\n")

    def info(self, msg: str):    self._write(f"â„¹ {msg}")
    def success(self, msg: str): self._write(f"âœ“ {msg}")
    def warning(self, msg: str): self._write(f"âš  {msg}")
    def error(self, msg: str):   self._write(f"âœ— {msg}")

    def api_call(self, tokens: int = 0):
        self.api_calls += 1
        self.tokens_used += tokens

    def api_failure(self):
        self.api_failures += 1

    def summary(self):
        elapsed = (datetime.now() - self.start_time).total_seconds()
        self._write(f"\n{'=' * 60}")
        self._write(f"ä¼šè¯æ€»ç»“:")
        self._write(f"  è€—æ—¶: {elapsed:.1f}ç§’")
        self._write(f"  APIè°ƒç”¨: {self.api_calls}æ¬¡ (å¤±è´¥{self.api_failures}æ¬¡)")
        self._write(f"  Tokenæ¶ˆè€—: ~{self.tokens_used}")
        self._write(f"{'=' * 60}")


class TranslationCache:
    def __init__(self, cache_file: str, logger: TranslationLogger):
        self.cache_file = cache_file
        self.logger = logger
        self.data = self._load()
        self.dirty = False

    def _load(self) -> dict:
        if not os.path.exists(self.cache_file):
            return {"_meta": {"version": 3, "created": datetime.now().isoformat()},
                    "translations": {}}
        try:
            with open(self.cache_file, "r", encoding="utf-8") as f:
                data = json.load(f)
            if "_meta" not in data:
                self.logger.warning("æ£€æµ‹åˆ°v1ç¼“å­˜æ ¼å¼ï¼Œè‡ªåŠ¨è¿ç§»")
                return {
                    "_meta": {"version": 3, "migrated": datetime.now().isoformat()},
                    "translations": {f"_legacy::{k}": v for k, v in data.items()}
                }
            return data
        except json.JSONDecodeError:
            self.logger.error(f"ç¼“å­˜æ–‡ä»¶æŸå: {self.cache_file}")
            bak = self.cache_file + ".bak"
            if os.path.exists(bak):
                self.logger.info("å°è¯•ä»å¤‡ä»½æ¢å¤ç¼“å­˜...")
                try:
                    with open(bak, "r", encoding="utf-8") as f:
                        return json.load(f)
                except Exception:
                    pass
            self.logger.warning("åˆ›å»ºæ–°ç¼“å­˜")
            return {"_meta": {"version": 3}, "translations": {}}

    def _make_key(self, category: str, name: str) -> str:
        return f"{category}::{name}"

    def get(self, category: str, name: str) -> str | None:
        key = self._make_key(category, name)
        result = self.data["translations"].get(key)
        if result is None:
            legacy_key = f"_legacy::{name}"
            result = self.data["translations"].get(legacy_key)
        return result

    def put(self, category: str, name: str, translation: str):
        key = self._make_key(category, name)
        self.data["translations"][key] = translation
        self.dirty = True

    def save(self):
        if not self.dirty:
            return
        self.data["_meta"]["last_saved"] = datetime.now().isoformat()
        self.data["_meta"]["count"] = len(self.data["translations"])
        tmp_file = self.cache_file + ".tmp"
        bak_file = self.cache_file + ".bak"
        try:
            with open(tmp_file, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
            with open(tmp_file, "r", encoding="utf-8") as f:
                json.load(f)
            if os.path.exists(self.cache_file):
                shutil.copy2(self.cache_file, bak_file)
            shutil.move(tmp_file, self.cache_file)
            self.dirty = False
        except Exception as e:
            self.logger.error(f"ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            if os.path.exists(tmp_file):
                os.remove(tmp_file)

    @property
    def size(self) -> int:
        return len(self.data["translations"])


def compute_file_hash(filepath: str) -> str:
    h = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(8192), b""):
            h.update(chunk)
    return h.hexdigest()


def backup_file(filepath: str, backup_dir: str, logger: TranslationLogger):
    os.makedirs(backup_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = Path(filepath).name
    backup_path = os.path.join(backup_dir, f"{filename}.{timestamp}.bak")
    shutil.copy2(filepath, backup_path)
    logger.info(f"å·²å¤‡ä»½: {backup_path}")
    return backup_path


def extract_field_values(content: str, field_name: str, logger: TranslationLogger) -> list[str]:
    """æå–æŒ‡å®šå­—æ®µçš„æ‰€æœ‰å”¯ä¸€å€¼"""
    pattern = rf'''{field_name}:\s*(?:"((?:[^"\\]|\\.)*)"|'((?:[^'\\]|\\.)*)')'''
    matches = re.findall(pattern, content)

    seen = set()
    unique = []
    for m in matches:
        val = m[0] if m[0] else m[1]
        val = val.replace('\\"', '"').replace("\\'", "'").replace('\\\\', '\\')
        if val and val not in seen:
            seen.add(val)
            unique.append(val)

    return unique


def extract_names_by_section(content: str, logger: TranslationLogger) -> dict[str, list[str]]:
    """æå– Name å­—æ®µï¼ˆæŒ‰ section åˆ†ç±»ï¼‰"""
    sections_config = [
        ("GlobalColors", "å…¨å±€é¢œè‰²/ç¯å¢ƒåç§°"),
        ("Tiles", "æ–¹å—/å›¾æ ¼åç§°"),
        ("Walls", "å¢™å£åç§°"),
        ("Items", "ç‰©å“åç§°"),
        ("Npcs", "NPCåç§°"),
        ("ItemPrefix", "ç‰©å“å‰ç¼€/ä¿®é¥°è¯"),
    ]

    categories = {}

    for section_key, category_name in sections_config:
        pattern = rf'{section_key}\s*:\s*\['
        match = re.search(pattern, content)
        if not match:
            logger.warning(f"æœªæ‰¾åˆ° section: {section_key}")
            continue

        start = match.end()
        bracket_depth = 1
        pos = start
        in_string = False
        string_char = None
        escape_next = False

        while pos < len(content) and bracket_depth > 0:
            ch = content[pos]
            if escape_next:
                escape_next = False
                pos += 1
                continue
            if ch == '\\':
                escape_next = True
                pos += 1
                continue
            if in_string:
                if ch == string_char:
                    in_string = False
            else:
                if ch in ('"', "'"):
                    in_string = True
                    string_char = ch
                elif ch == '[':
                    bracket_depth += 1
                elif ch == ']':
                    bracket_depth -= 1
            pos += 1

        section_content = content[start:pos - 1]
        name_pattern = r'''Name:\s*(?:"((?:[^"\\]|\\.)*)"|'((?:[^'\\]|\\.)*)')'''
        matches = re.findall(name_pattern, section_content)

        seen = set()
        unique_names = []
        for m in matches:
            name = m[0] if m[0] else m[1]
            name = name.replace('\\"', '"').replace("\\'", "'").replace('\\\\', '\\')
            if name not in seen:
                seen.add(name)
                unique_names.append(name)

        categories[category_name] = unique_names
        logger.info(f"  {category_name} ({section_key}): {len(unique_names)} æ¡å”¯ä¸€åç§°")

    return categories


def extract_extra_fields(content: str, logger: TranslationLogger) -> dict[str, list[str]]:
    """æå– Name ä»¥å¤–çš„å¯ç¿»è¯‘å­—æ®µï¼ˆAnchor, Variety ç­‰ï¼‰"""
    extra_categories = {}

    for field_name, cache_prefix, desc in TRANSLATABLE_FIELDS:
        if field_name == "Name":
            continue  # Name ç”± extract_names_by_section å•ç‹¬å¤„ç†

        values = extract_field_values(content, field_name, logger)

        # è¿‡æ»¤æ‰é™æ€è¯å…¸å·²è¦†ç›–çš„
        need_translate = [v for v in values if v not in STATIC_DICT]
        static_hit = len(values) - len(need_translate)

        category_key = f"{desc}ï¼ˆ{field_name}ï¼‰"
        extra_categories[category_key] = need_translate

        logger.info(f"  {category_key}: {len(values)} æ¡å”¯ä¸€å€¼"
                     f"ï¼ˆé™æ€è¯å…¸å‘½ä¸­ {static_hit}ï¼Œéœ€APIç¿»è¯‘ {len(need_translate)}ï¼‰")

    return extra_categories


def validate_translation_response(
    names: list[str],
    translations: dict[str, str],
    logger: TranslationLogger
) -> dict[str, str]:
    validated = {}
    issues = []

    for name in names:
        if name not in translations:
            issues.append(f"ç¼ºå¤±ç¿»è¯‘: '{name}'")
            validated[name] = name
            continue

        translated = translations[name]

        if name and not translated:
            issues.append(f"ç©ºç¿»è¯‘: '{name}' â†’ ''")
            validated[name] = name
            continue

        dangerous_patterns = ['\n', '\r', '\x00']
        has_danger = False
        for dp in dangerous_patterns:
            if dp in translated:
                issues.append(f"å±é™©å­—ç¬¦: '{name}' â†’ '{translated[:50]}'")
                has_danger = True
                break
        if has_danger:
            validated[name] = name
            continue

        if len(name) > 3 and len(translated) > len(name) * 5:
            issues.append(f"é•¿åº¦å¼‚å¸¸: '{name}'({len(name)}) â†’ '{translated[:50]}'({len(translated)})")

        # æ£€æŸ¥æ‹¬å·ä¸¢å¤±å¹¶è‡ªåŠ¨è¡¥å…¨
        if '(' in name and '(' not in translated and 'ï¼ˆ' not in translated:
            paren_match = re.search(r'\(([^)]*)\)', name)
            if paren_match:
                inner = paren_match.group(1)
                translated = f"{translated}ï¼ˆ{inner}ï¼‰"
                issues.append(f"æ‹¬å·è¡¥å…¨: '{name}' â†’ '{translated}'")

        validated[name] = translated

    if issues:
        logger.warning(f"æ ¡éªŒå‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues[:10]:
            logger.warning(f"  - {issue}")
        if len(issues) > 10:
            logger.warning(f"  ... å…± {len(issues)} ä¸ª")

    return validated


def parse_ai_response(response_text: str, names: list[str], logger: TranslationLogger) -> dict[str, str] | None:
    text = response_text.strip()

    # ç­–ç•¥1: ç›´æ¥è§£æ
    try:
        result = json.loads(text)
        parsed = _extract_from_parsed(result, names)
        if parsed and len(parsed) >= len(names) * 0.5:
            return parsed
    except json.JSONDecodeError:
        pass

    # ç­–ç•¥2: markdown ä»£ç å—
    json_match = re.search(r'```(?:json)?\s*([\s\S]*?)```', text)
    if json_match:
        try:
            result = json.loads(json_match.group(1))
            parsed = _extract_from_parsed(result, names)
            if parsed:
                return parsed
        except json.JSONDecodeError:
            pass

    # ç­–ç•¥3: æ‰¾ { } èŒƒå›´
    first_brace = text.find('{')
    last_brace = text.rfind('}')
    if first_brace >= 0 and last_brace > first_brace:
        try:
            result = json.loads(text[first_brace:last_brace + 1])
            parsed = _extract_from_parsed(result, names)
            if parsed:
                return parsed
        except json.JSONDecodeError:
            pass

    # ç­–ç•¥4: ä¿®å¤å¸¸è§ JSON é”™è¯¯
    try:
        cleaned = re.sub(r',\s*([}\]])', r'\1', text)
        cleaned = re.sub(r"'", '"', cleaned)
        first_brace = cleaned.find('{')
        last_brace = cleaned.rfind('}')
        if first_brace >= 0 and last_brace > first_brace:
            result = json.loads(cleaned[first_brace:last_brace + 1])
            parsed = _extract_from_parsed(result, names)
            if parsed:
                logger.warning("é€šè¿‡ JSON ä¿®å¤ç­–ç•¥è§£ææˆåŠŸ")
                return parsed
    except Exception:
        pass

    # ç­–ç•¥5: ä»æ€ç»´é“¾æå–
    json_blocks = re.findall(r'\{[^{}]*"translations"[^{}]*\[[\s\S]*?\]\s*\}', text)
    for block in json_blocks:
        try:
            result = json.loads(block)
            parsed = _extract_from_parsed(result, names)
            if parsed:
                logger.warning("ä»æ€ç»´é“¾ä¸­æå–åˆ°ç¿»è¯‘JSON")
                return parsed
        except json.JSONDecodeError:
            continue

    logger.error(f"æ‰€æœ‰è§£æç­–ç•¥å‡å¤±è´¥ï¼ŒåŸå§‹è¿”å›å‰300å­—ç¬¦: {text[:300]}")
    return None


def _extract_from_parsed(result: dict, names: list[str]) -> dict[str, str] | None:
    translations = {}

    if "translations" in result and isinstance(result["translations"], list):
        for item in result["translations"]:
            if isinstance(item, dict):
                en = item.get("en", item.get("original", item.get("name", "")))
                zh = item.get("zh", item.get("translation", item.get("cn", "")))
                if en:
                    translations[en] = zh
        if translations:
            return translations

    if all(isinstance(v, str) for v in result.values()):
        return dict(result)

    for key in ("results", "data", "output"):
        if key in result and isinstance(result[key], dict):
            sub = result[key]
            if all(isinstance(v, str) for v in sub.values()):
                return dict(sub)

    return translations if translations else None


def translate_batch_safe(
    client: OpenAI,
    names: list[str],
    category: str,
    logger: TranslationLogger,
    cache: TranslationCache
) -> dict[str, str]:
    system_prompt = f"""ä½ æ˜¯æ³°æ‹‰ç‘äºš(Terraria)æ¸¸æˆçš„ä¸“ä¸šç¿»è¯‘å‘˜ã€‚è¯·å°†æ¸¸æˆä¸­çš„è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ã€‚

å½“å‰ç¿»è¯‘çš„ç±»åˆ«æ˜¯ï¼šã€Œ{category}ã€

è¦æ±‚ï¼š
1. ä½¿ç”¨æ³°æ‹‰ç‘äºšå®˜æ–¹ä¸­æ–‡ç¿»è¯‘ï¼ˆå‚è€ƒSteamä¸­æ–‡ç‰ˆ/Wikiï¼‰
2. ä¿æŒä¸“æœ‰åè¯å‡†ç¡®æ€§
3. ç©ºå­—ç¬¦ä¸²ä¿æŒä¸ºç©º
4. æ•°é‡å¿…é¡»ä¸è¾“å…¥å®Œå…¨ä¸€è‡´ï¼Œé¡ºåºä¸€è‡´
5. å¦‚æœåŸæ–‡åŒ…å«æ‹¬å·å¦‚ "Name (Variant)"ï¼Œç¿»è¯‘åå¿…é¡»ä¿ç•™æ‹¬å·ï¼Œä½¿ç”¨ä¸­æ–‡æ‹¬å·ï¼Œå¦‚ "åç§°ï¼ˆå˜ä½“ï¼‰"
6. å¯¹äºæ–¹ä½è¯å¦‚ Top/Bottom/Left/Right ç¿»è¯‘ä¸º é¡¶éƒ¨/åº•éƒ¨/å·¦ä¾§/å³ä¾§
7. å¯¹äºæè¿°æ€§çŸ­è¯­è¦ç¿»è¯‘å®Œæ•´ï¼Œå¦‚ "Right Indent A" â†’ "å³ç¼©è¿› A"

ä¸¥æ ¼è¿”å›JSONæ ¼å¼ï¼š
{{"translations": [{{"en": "åŸæ–‡1", "zh": "è¯‘æ–‡1"}}, {{"en": "åŸæ–‡2", "zh": "è¯‘æ–‡2"}}]}}

æ³¨æ„ï¼šåªè¿”å›JSONï¼Œä¸è¦è¿”å›ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

    names_list = "\n".join(f"{i + 1}. {name}" for i, name in enumerate(names))
    user_prompt = f"ç¿»è¯‘ä»¥ä¸‹æ³°æ‹‰ç‘äºšçš„ã€Œ{category}ã€ï¼ˆå…±{len(names)}æ¡ï¼‰ï¼š\n\n{names_list}"

    last_error = None

    for attempt in range(MAX_RETRIES):
        try:
            logger.info(f"    APIè°ƒç”¨ (å°è¯• {attempt + 1}/{MAX_RETRIES})...")

            response = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.1,
                max_tokens=MAX_TOKENS,
                timeout=120,
            )

            tokens = 0
            if response.usage:
                tokens = response.usage.total_tokens
            logger.api_call(tokens)

            raw_text = get_message_content(response.choices[0].message)
            if not raw_text:
                raise ValueError("APIè¿”å›ç©ºå†…å®¹")

            logger.info(f"    æ”¶åˆ°å“åº”: {len(raw_text)} å­—ç¬¦, {tokens} tokens")

            translations = parse_ai_response(raw_text, names, logger)
            if translations is None:
                raise ValueError("æ— æ³•è§£æAPIè¿”å›")

            validated = validate_translation_response(names, translations, logger)

            coverage = sum(1 for n in names if n in validated and validated[n] != n) / max(len(names), 1)
            if coverage < 0.3 and len(names) > 5:
                logger.warning(f"    ç¿»è¯‘è¦†ç›–ç‡è¿‡ä½: {coverage:.0%}")
                if attempt < MAX_RETRIES - 1:
                    logger.info(f"    é‡è¯•ä»¥è·å–æ›´å¥½çš„ç»“æœ...")
                    time.sleep(RETRY_BASE_DELAY)
                    continue

            return validated

        except Exception as e:
            last_error = e
            logger.api_failure()
            logger.error(f"    å¤±è´¥: {type(e).__name__}: {e}")
            if attempt < MAX_RETRIES - 1:
                delay = RETRY_BASE_DELAY * (2 ** attempt)
                logger.info(f"    ç­‰å¾… {delay}s åé‡è¯•...")
                time.sleep(delay)

    logger.error(f"  æ‰¹æ¬¡ç¿»è¯‘å½»åº•å¤±è´¥ ({last_error})ï¼Œä½¿ç”¨é™çº§ç­–ç•¥")

    if len(names) > 10:
        logger.info(f"  é™çº§: æ‹†åˆ†ä¸ºæ¯æ‰¹10æ¡é‡è¯•...")
        result = {}
        for i in range(0, len(names), 10):
            sub_batch = names[i:i + 10]
            try:
                sub_result = translate_batch_safe(client, sub_batch, category, logger, cache)
                result.update(sub_result)
                for name in sub_batch:
                    if name in sub_result:
                        cache.put(category, name, sub_result[name])
                cache.save()
                time.sleep(REQUEST_INTERVAL)
            except Exception:
                for name in sub_batch:
                    result[name] = name
        return result

    return {name: name for name in names}


def replace_field_values(
    content: str,
    field_name: str,
    translation_map: dict[str, str],
    logger: TranslationLogger
) -> str:
    """æ›¿æ¢æŒ‡å®šå­—æ®µçš„å€¼"""
    replacement_count = 0

    def replace_match(match):
        nonlocal replacement_count
        full_match = match.group(0)
        prefix = match.group(1)
        quote = match.group(2)
        original = match.group(3)

        if quote == '"':
            real_name = original.replace('\\"', '"').replace('\\\\', '\\')
        else:
            real_name = original.replace("\\'", "'").replace('\\\\', '\\')

        translated = translation_map.get(real_name)
        if translated is None or translated == real_name:
            return full_match

        if quote == '"':
            escaped = translated.replace('\\', '\\\\').replace('"', '\\"')
        else:
            escaped = translated.replace('\\', '\\\\').replace("'", "\\'")

        replacement_count += 1
        return f'{prefix}{quote}{escaped}{quote}'

    pattern = rf'''({field_name}:\s*)(["'])((?:(?!\2|\\).|\\.)*)\2'''
    result = re.sub(pattern, replace_match, content)

    logger.info(f"  å­—æ®µ {field_name}: æ›¿æ¢ {replacement_count} å¤„")
    return result


def verify_js_integrity(original: str, modified: str, logger: TranslationLogger) -> bool:
    issues = []

    # èŠ±æ‹¬å·ã€æ–¹æ‹¬å·ï¼šä¸¥æ ¼æ£€æŸ¥
    for char, counter_char, name in [
        ('{', '}', 'èŠ±æ‹¬å·'),
        ('[', ']', 'æ–¹æ‹¬å·'),
    ]:
        orig_open = original.count(char)
        orig_close = original.count(counter_char)
        mod_open = modified.count(char)
        mod_close = modified.count(counter_char)
        if orig_open != mod_open or orig_close != mod_close:
            issues.append(f"{name}æ•°é‡å˜åŒ–: åŸæ–‡({orig_open}/{orig_close}) â†’ ä¿®æ”¹å({mod_open}/{mod_close})")
        else:
            logger.info(f"  âœ“ {name}: {orig_open}/{orig_close}")

    # åŠè§’åœ†æ‹¬å·ï¼šå…è®¸å‡å°‘ï¼ˆç¿»è¯‘ä¸ºå…¨è§’æ˜¯æ­£å¸¸è¡Œä¸ºï¼‰
    orig_po, orig_pc = original.count('('), original.count(')')
    mod_po, mod_pc = modified.count('('), modified.count(')')
    if mod_po > orig_po or mod_pc > orig_pc:
        issues.append(f"åŠè§’åœ†æ‹¬å·å¼‚å¸¸å¢åŠ : ({orig_po}/{orig_pc}) â†’ ({mod_po}/{mod_pc})")
    elif mod_po < orig_po:
        lost = orig_po - mod_po
        logger.info(f"  âœ“ åŠè§’åœ†æ‹¬å·: {orig_po} â†’ {mod_po}ï¼ˆ{lost}ä¸ªè¢«ç¿»è¯‘ä¸ºå…¨è§’ï¼Œæ­£å¸¸ï¼‰")
    else:
        logger.info(f"  âœ“ åŠè§’åœ†æ‹¬å·: {orig_po}/{orig_pc}")

    # å…³é”®ç»“æ„
    for keyword in ["var settings", "GlobalColors", "Tiles", "Walls", "Items", "Npcs",
                     "ItemPrefix", "hexToRgb", "function"]:
        if keyword in original and keyword not in modified:
            issues.append(f"å…³é”®ç»“æ„ä¸¢å¤±: {keyword}")

    structural_ok = all(kw in modified for kw in
                        ["var settings", "GlobalColors", "Tiles", "Walls", "Items", "Npcs", "ItemPrefix"]
                        if kw in original)
    if structural_ok:
        logger.info(f"  âœ“ å…³é”®ç»“æ„å®Œæ•´")

    # Name å­—æ®µæ•°é‡
    orig_names = len(re.findall(r'Name:', original))
    mod_names = len(re.findall(r'Name:', modified))
    if orig_names != mod_names:
        issues.append(f"Nameå­—æ®µæ•°é‡å˜åŒ–: {orig_names} â†’ {mod_names}")
    else:
        logger.info(f"  âœ“ Nameå­—æ®µ: {orig_names}")

    # å…¶ä»–ç¿»è¯‘å­—æ®µæ•°é‡
    for field_name, _, _ in TRANSLATABLE_FIELDS:
        if field_name == "Name":
            continue
        orig_c = len(re.findall(rf'{field_name}:', original))
        mod_c = len(re.findall(rf'{field_name}:', modified))
        if orig_c != mod_c:
            issues.append(f"{field_name}å­—æ®µæ•°é‡å˜åŒ–: {orig_c} â†’ {mod_c}")
        elif orig_c > 0:
            logger.info(f"  âœ“ {field_name}å­—æ®µ: {orig_c}")

    # æ–‡ä»¶å¤§å°
    size_ratio = len(modified) / max(len(original), 1)
    if size_ratio < 0.5 or size_ratio > 3.0:
        issues.append(f"æ–‡ä»¶å¤§å°å¼‚å¸¸: åŸ{len(original)} â†’ ç°{len(modified)} (æ¯”ç‡{size_ratio:.2f})")
    else:
        logger.info(f"  âœ“ æ–‡ä»¶å¤§å°: {len(original):,} â†’ {len(modified):,} (Ã—{size_ratio:.2f})")

    if issues:
        logger.error(f"å®Œæ•´æ€§æ ¡éªŒå‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues:
            logger.error(f"  âœ— {issue}")
        return False

    logger.success("å®Œæ•´æ€§æ ¡éªŒé€šè¿‡ âœ“")
    return True


def generate_review_file(
    all_categories: dict[str, list[str]],
    all_maps: dict[str, dict[str, str]],
    review_file: str,
    logger: TranslationLogger
):
    review_data = {
        "_è¯´æ˜": "æ­¤æ–‡ä»¶ç”¨äºäººå·¥å®¡æŸ¥ç¿»è¯‘è´¨é‡",
        "_ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
        "categories": {}
    }

    for category, names in all_categories.items():
        tmap = all_maps.get(category, {})
        items = []
        for name in names:
            translated = tmap.get(name, name)
            item = {"en": name, "zh": translated}
            if name == translated and name:
                item["_status"] = "æœªç¿»è¯‘"
            items.append(item)

        review_data["categories"][category] = {
            "total": len(items),
            "translated": sum(1 for it in items if it.get("_status") != "æœªç¿»è¯‘" or not it["en"]),
            "items": items
        }

    with open(review_file, "w", encoding="utf-8") as f:
        json.dump(review_data, f, ensure_ascii=False, indent=2)
    logger.info(f"å®¡æŸ¥æ–‡ä»¶å·²ç”Ÿæˆ: {review_file}")


def translate_category(
    client: OpenAI,
    category: str,
    names: list[str],
    cache: TranslationCache,
    logger: TranslationLogger,
    static_dict: dict[str, str] | None = None,
) -> dict[str, str]:
    """ç¿»è¯‘å•ä¸ªç±»åˆ«ï¼Œè¿”å› {åŸæ–‡: è¯‘æ–‡} æ˜ å°„"""
    translation_map = {}

    if category in SKIP_CATEGORIES:
        logger.info(f"è·³è¿‡ç±»åˆ«: {category}")
        for name in names:
            translation_map[name] = name
        return translation_map

    logger.info(f"\n{'â”€' * 50}")
    logger.info(f"ğŸ“ ç±»åˆ«: {category} ({len(names)} æ¡)")

    to_translate = []
    for name in names:
        # 1. å…ˆæŸ¥é™æ€è¯å…¸
        if static_dict and name in static_dict:
            translation_map[name] = static_dict[name]
            continue
        # 2. å†æŸ¥ç¼“å­˜
        cached = cache.get(category, name)
        if cached is not None:
            translation_map[name] = cached
        else:
            to_translate.append(name)

    static_hits = sum(1 for n in names if static_dict and n in static_dict) if static_dict else 0
    cache_hits = len(names) - len(to_translate) - static_hits
    logger.info(f"  é™æ€è¯å…¸: {static_hits}, ç¼“å­˜å‘½ä¸­: {cache_hits}, éœ€APIç¿»è¯‘: {len(to_translate)}")

    if not to_translate:
        logger.success(f"  å…¨éƒ¨å·²æœ‰ç¿»è¯‘ âœ“")
        return translation_map

    total_batches = (len(to_translate) + BATCH_SIZE - 1) // BATCH_SIZE

    for i in range(0, len(to_translate), BATCH_SIZE):
        batch = to_translate[i:i + BATCH_SIZE]
        batch_num = i // BATCH_SIZE + 1

        logger.info(f"  æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)}æ¡)")

        translations = translate_batch_safe(client, batch, category, logger, cache)

        for name in batch:
            translated = translations.get(name, name)
            translation_map[name] = translated
            cache.put(category, name, translated)

        cache.save()
        logger.success(f"  æ‰¹æ¬¡ {batch_num} å®Œæˆï¼Œç¼“å­˜å·²ä¿å­˜")

        samples = [(n, translations.get(n, n)) for n in batch[:5]]
        for en, zh in samples:
            marker = "âœ“" if en != zh else "â—‹"
            logger.info(f"    {marker} ã€Œ{en}ã€â†’ã€Œ{zh}ã€")
        if len(batch) > 5:
            tc = sum(1 for n in batch if translations.get(n, n) != n)
            logger.info(f"    ... å…± {len(batch)} æ¡, å·²ç¿»è¯‘ {tc} æ¡")

        if i + BATCH_SIZE < len(to_translate):
            time.sleep(REQUEST_INTERVAL)

    return translation_map


def main():
    logger = TranslationLogger(LOG_FILE)
    logger.info("ğŸ® æ³°æ‹‰ç‘äºš settings.js æ±‰åŒ–å·¥å…· (æœ€ç»ˆç‰ˆ v2)")
    logger.info(f"é…ç½®: MODEL={MODEL}")
    logger.info(f"é…ç½®: BATCH_SIZE={BATCH_SIZE}, MODE={TRANSLATION_MODE}")
    logger.info(f"ç¿»è¯‘å­—æ®µ: {', '.join(f[0] for f in TRANSLATABLE_FIELDS)}")

    # ---- å‰ç½®æ£€æŸ¥ ----
    if not os.path.exists(INPUT_FILE):
        logger.error(f"æ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶: {INPUT_FILE}")
        logger.error(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        sys.exit(1)

    if not API_KEY:
        logger.error("è¯·è®¾ç½® API_KEYï¼ˆç¯å¢ƒå˜é‡ OPENAI_API_KEY æˆ–ä¿®æ”¹è„šæœ¬é…ç½®åŒºï¼‰")
        sys.exit(1)

    # ---- å¤‡ä»½ ----
    file_hash = compute_file_hash(INPUT_FILE)
    logger.info(f"è¾“å…¥æ–‡ä»¶ MD5: {file_hash}")
    backup_path = backup_file(INPUT_FILE, BACKUP_DIR, logger)

    # ---- è¯»å–æ–‡ä»¶ ----
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        content = f.read()
    logger.info(f"æ–‡ä»¶å¤§å°: {len(content):,} å­—ç¬¦, {content.count(chr(10)) + 1:,} è¡Œ")

    # ---- æå–æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„å†…å®¹ ----
    logger.info("\næå–å„å­—æ®µå†…å®¹...")

    # Name å­—æ®µï¼ˆæŒ‰ section ç»†åˆ†ï¼‰
    logger.info("â”€ Name å­—æ®µï¼ˆæŒ‰ section åˆ†ç±»ï¼‰:")
    name_categories = extract_names_by_section(content, logger)

    # å…¶ä»–å­—æ®µï¼ˆAnchor, Variety ç­‰ï¼‰
    logger.info("â”€ å…¶ä»–å¯ç¿»è¯‘å­—æ®µ:")
    extra_categories = extract_extra_fields(content, logger)

    total_unique = sum(len(v) for v in name_categories.values()) + sum(len(v) for v in extra_categories.values())
    static_covered = sum(1 for cat_vals in extra_categories.values() for v in cat_vals if v in STATIC_DICT)
    logger.info(f"\nå…±è®¡ {total_unique} æ¡å”¯ä¸€æ–‡æœ¬ï¼ˆé™æ€è¯å…¸è¦†ç›– {len(STATIC_DICT)} è¯ï¼‰")

    if DRY_RUN:
        logger.info("\nDRY_RUN æ¨¡å¼ï¼Œå±•ç¤ºæå–ç»“æœ:")
        for cat, names in {**name_categories, **extra_categories}.items():
            logger.info(f"  {cat}:")
            for n in names[:10]:
                static_mark = " [é™æ€]" if n in STATIC_DICT else ""
                logger.info(f"    - {n}{static_mark}")
            if len(names) > 10:
                logger.info(f"    ... ç­‰ {len(names)} æ¡")
        return

    # ---- åˆå§‹åŒ– ----
    cache = TranslationCache(CACHE_FILE, logger)
    logger.info(f"ç¼“å­˜å·²åŠ è½½: {cache.size} æ¡")

    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

    # ---- æµ‹è¯• API ----
    logger.info("æµ‹è¯• API è¿é€šæ€§...")
    try:
        test_resp = client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user",
                       "content": 'å°† "Dirt Block" ç¿»è¯‘ä¸ºæ³°æ‹‰ç‘äºšä¸­æ–‡ï¼Œåªå›å¤ï¼š{"zh":"è¯‘æ–‡"}'}],
            max_tokens=MAX_TOKENS,
            timeout=30,
        )
        test_content = get_message_content(test_resp.choices[0].message).strip()
        if test_content:
            zh_match = re.search(r'[\u4e00-\u9fff]+', test_content)
            display = zh_match.group(0) if zh_match else test_content[:100]
            logger.success(f"API è¿é€š: Dirt Block â†’ {display}")
        else:
            logger.warning("API è¿é€šä½†è¿”å›ä¸ºç©ºï¼Œç»§ç»­...")
        if test_resp.usage:
            logger.api_call(test_resp.usage.total_tokens)
    except Exception as e:
        logger.error(f"API è¿é€šæµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)

    # ---- ç¿»è¯‘æ‰€æœ‰ç±»åˆ« ----
    # åˆå¹¶æ‰€æœ‰ç¿»è¯‘æ˜ å°„ï¼ˆæŒ‰å­—æ®µåˆ†ç»„ï¼‰
    all_categories = {}   # ç”¨äºå®¡æŸ¥æ–‡ä»¶
    field_maps = {}       # field_name â†’ {åŸæ–‡: è¯‘æ–‡}

    # ç¿»è¯‘ Name å­—æ®µ
    name_map = {}
    for category, names in name_categories.items():
        cat_map = translate_category(client, category, names, cache, logger)
        name_map.update(cat_map)
        all_categories[category] = names
    field_maps["Name"] = name_map

    # ç¿»è¯‘å…¶ä»–å­—æ®µ
    for field_name, cache_prefix, desc in TRANSLATABLE_FIELDS:
        if field_name == "Name":
            continue

        category_key = f"{desc}ï¼ˆ{field_name}ï¼‰"
        names = extra_categories.get(category_key, [])

        if not names:
            logger.info(f"\n{category_key}: æ— éœ€APIç¿»è¯‘çš„å†…å®¹")
            # é™æ€è¯å…¸çš„ä¹Ÿè¦æ”¾è¿› field_maps
            field_maps[field_name] = dict(STATIC_DICT)
            continue

        # ç”¨å¸¦å‰ç¼€çš„ç¼“å­˜åˆ†ç±»é¿å…è·¨å­—æ®µå†²çª
        actual_category = f"{cache_prefix}_{desc}" if cache_prefix else desc

        cat_map = translate_category(
            client, category_key, names, cache, logger,
            static_dict=STATIC_DICT
        )

        # åˆå¹¶é™æ€è¯å…¸
        merged = dict(STATIC_DICT)
        merged.update(cat_map)
        field_maps[field_name] = merged
        all_categories[category_key] = names

    # ---- æ›¿æ¢æ–‡ä»¶å†…å®¹ ----
    logger.info(f"\n{'â”€' * 50}")
    logger.info("æ›¿æ¢å„å­—æ®µå€¼...")
    new_content = content

    for field_name, _, _ in TRANSLATABLE_FIELDS:
        tmap = field_maps.get(field_name, {})
        if tmap:
            new_content = replace_field_values(new_content, field_name, tmap, logger)

    # ---- å®Œæ•´æ€§æ ¡éªŒ ----
    logger.info("\næ‰§è¡Œå®Œæ•´æ€§æ ¡éªŒ...")
    integrity_ok = verify_js_integrity(content, new_content, logger)

    if not integrity_ok:
        logger.error("å®Œæ•´æ€§æ ¡éªŒå¤±è´¥ï¼")
        emergency_output = OUTPUT_FILE + ".UNSAFE.js"
        with open(emergency_output, "w", encoding="utf-8") as f:
            f.write(new_content)
        logger.error(f"ç»“æœå·²ä¿å­˜åˆ° {emergency_output}ï¼Œè¯·äººå·¥æ£€æŸ¥")
        logger.warning(f"åŸæ–‡ä»¶å¤‡ä»½åœ¨: {backup_path}")
    else:
        with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
            f.write(new_content)
        logger.success(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")

    # ---- å®¡æŸ¥æ–‡ä»¶ ----
    all_maps = {}
    for cat in all_categories:
        # æ‰¾å¯¹åº”çš„ field_map
        for fn, _, desc in TRANSLATABLE_FIELDS:
            category_key = f"{desc}ï¼ˆ{fn}ï¼‰"
            if cat == category_key:
                all_maps[cat] = field_maps.get(fn, {})
                break
        else:
            all_maps[cat] = field_maps.get("Name", {})

    generate_review_file(all_categories, all_maps, REVIEW_FILE, logger)

    # ---- æœ€ç»ˆç»Ÿè®¡ ----
    cn_pattern = re.compile(r'[\u4e00-\u9fff]')
    stats = []
    for field_name, _, desc in TRANSLATABLE_FIELDS:
        field_values = re.findall(
            rf'''{field_name}:\s*["']([^"']*?)["']''', new_content
        )
        total = len(field_values)
        cn = sum(1 for v in field_values if cn_pattern.search(v))
        if total > 0:
            stats.append((field_name, desc, total, cn))

    logger.info(f"\n{'â•' * 50}")
    logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
    for field_name, desc, total, cn in stats:
        rate = cn / max(total, 1) * 100
        logger.info(f"  {field_name}({desc}): {total}å¤„, å«ä¸­æ–‡{cn}å¤„, æ±‰åŒ–ç‡{rate:.1f}%")

    total_all = sum(s[2] for s in stats)
    cn_all = sum(s[3] for s in stats)
    logger.info(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.info(f"  æ€»è®¡: {total_all}å¤„, å«ä¸­æ–‡{cn_all}å¤„, æ±‰åŒ–ç‡{cn_all / max(total_all, 1) * 100:.1f}%")
    logger.info(f"  å®Œæ•´æ€§: {'âœ“ é€šè¿‡' if integrity_ok else 'âœ— å¤±è´¥'}")
    logger.summary()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš  ç”¨æˆ·ä¸­æ–­ï¼Œä¸‹æ¬¡è¿è¡Œå°†ä»ç¼“å­˜ç»§ç»­")
    except Exception as e:
        print(f"\n\nâœ— æœªé¢„æœŸçš„é”™è¯¯: {e}")
        traceback.print_exc()
        print("ç¼“å­˜æ–‡ä»¶åº”å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œå¯ç»§ç»­")